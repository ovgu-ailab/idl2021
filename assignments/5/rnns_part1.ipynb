{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnns_part1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMNRrNJS3Dbr71CnFVK3jax"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"r-Chp8YyyvxN"},"source":["import tensorflow as tf\n","import numpy as np\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVE7aMvC4iXK"},"source":["# remove infrequent words. you can play with this parameter as it will likely impact model quality\n","num_words = 20000\n","(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c07u7Z7s4opk"},"source":["# look at some sequences. words have been replaced with arbitrary index mappings\n","# 1 is a special \"beginning of sequence\" marker\n","# infrequent words have been replaced by the index 2\n","# actual words start with index 4, 3 is never used (???)\n","train_sequences[:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AD6Elit34sTL"},"source":["# labels are simply binary: sentiment can be positive or negative\n","train_labels[:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHTMEyXW5KcQ"},"source":["# to restore words, load the word-to-index mapping\n","word_to_index = tf.keras.datasets.imdb.get_word_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vv25lUc_5ckG"},"source":["# invert to get index-to-word mapping\n","index_to_word = dict((index, word) for (word, index) in word_to_index.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYX6F3AX5hpV"},"source":["# we can convert a sequence to text by\n","# - replacing each index by the respective word\n","# - joining words together via spaces\n","# note that we remove the beginning of sequence character and we have to subtract 3 from all indices\n","# this is because, as mentioned above, the smallest indices are reserved for special characters\n","# but for some reason this is not reflected in the mapping...\n","\" \".join([index_to_word[index - 3] for index in train_sequences[0][1:]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9axnbnwR6q6W"},"source":["# we cannot create a dataset :( this is because sequences are different length\n","# but tensors have to be \"rectangular\"\n","train_data = tf.data.Dataset.from_tensor_slices(train_sequences, train_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m2lt9mE-9XO7"},"source":["# solution is padding all sequences to the maximum length.\n","# first find the maximum length\n","sequence_lengths = [len(sequence) for sequence in train_sequences]\n","max_len = max(sequence_lengths)\n","max_len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"677ZXcRu9nUe"},"source":["# overview over sequence lengths in the data\n","# could also look at mean, median, standard deviation...\n","plt.hist(sequence_lengths, bins=80)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYr10G5M9rWX"},"source":["# luckily there is a convenient function for padding\n","train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXEICggj-OL-"},"source":["# now we can create a dataset!\n","train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPTPy5Ff-Q_C"},"source":["# all sequences are... very long\n","train_sequences_padded.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ug0OSIGjf6ji"},"source":["# it would be better to do something like this\n","# all sequences above maxlen will be truncated to that length\n","# note: pad_sequences has \"pre\" and \"post\" options for both padding and truncation. one may be better than the other!\n","train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=200)\n","train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n","\n","train_sequences_padded.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZW7YdDv_fRJ"},"source":["# for fun, you can look at the word-index mappings.\n","# in this case, the mapping was done according to word frequency/\n","# you can pass reverse=True to sorted() to look at the least common words.\n","sorted(index_to_word.items())[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4fwUhqBACri"},"source":["# here is a high-level sketch for training RNNs\n","\n","\n","# training loop -- same thing as before!!\n","# our data is now slightly different (each batch of sequences has a time axis, which is kinda new)\n","# but all the related changes are hidden away at lower levels\n","def train_loop():\n","    for sequence_batch, label_batch in train_data:\n","        train_step(sequence_batch, label_batch)\n","\n","\n","# a single training step -- again, seems familiar?\n","def train_step(sequences, labels):\n","    with tf.GradientTape() as tape:\n","        logits = rnn_loop(sequences)\n","        loss = loss_fn(labels, logits)\n","\n","    gradient = ...\n","    apply_gradients(...)\n","\n","\n","# here's where things start to change\n","# we loop over the input time axis, and at each time step compute the new\n","# hidden state based on the previous one as well as the current input\n","# the state computation is hidden away in the rnn_step function and could be\n","# arbitrarily complex.\n","# in the general RNN, an output is computed at each time step, and the whole\n","# sequence is returned. but in this case, since we only have one label for the\n","# entire sequence, we only use the final state to compute one output and return it.\n","# before the loop, the state need to be initialized somehow.\n","def rnn_loop(sequences):\n","    old_state = ...\n","\n","    for step in range(max_len):\n","        x_t = sequences[:, step]\n","        x_t = tf.one_hot(x_t, depth=num_words)\n","        new_state = rnn_step(old_state, x_t)\n","\n","        old_state = new_state\n","\n","    o_t = output_layer(new_state)\n","\n","    return o_t\n","\n","\n","# see formulas in the book ;)\n","def rnn_step(state, x_t):\n","    ..."],"execution_count":null,"outputs":[]}]}